{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b5b27e1",
   "metadata": {},
   "source": [
    "# 문제 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2f9f03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "마지막 줄 : 1426.0, \"FAG_60_ABOVE_SUNTM_5_FLPOP_CO\": 1914.0, \"FAG_60_ABOVE_SUNTM_6_FLPOP_CO\": 1325.0}]}}None\n",
      "읽은 건 수 : 6028\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import urllib\n",
    "from src import mylib\n",
    "import json\n",
    "import io\n",
    "\n",
    "\n",
    "keyPath=os.path.join(os.getcwd(), 'src', 'key.properties')\n",
    "key=mylib.getKey(keyPath)\n",
    "KEY=key['dataseoul']\n",
    "\n",
    "TYPE='json'\n",
    "SERVICE='VwsmTrdarFlpopQq'\n",
    "START_INDEX=str(1)\n",
    "END_INDEX=str(10)\n",
    "STDR_YM_CD=str(2020)\n",
    "params=\"/\".join([KEY,TYPE,SERVICE,START_INDEX,END_INDEX,STDR_YM_CD])\n",
    "\n",
    "_url='http://openAPI.seoul.go.kr:8088'\n",
    "url=\"/\".join([_url,params])\n",
    "\n",
    "\n",
    "r=requests.get(url)\n",
    "trade=r.json()\n",
    "\n",
    "with io.open('src/ds_practice_data.json', 'a', encoding='utf8') as json_file:\n",
    "    _j=json.dump(trade, json_file, ensure_ascii=False)\n",
    "    json_file.write(str(_j)+\"\\n\")     \n",
    "    \n",
    "lastLine='1426.0, \"FAG_60_ABOVE_SUNTM_5_FLPOP_CO\": 1914.0, \"FAG_60_ABOVE_SUNTM_6_FLPOP_CO\": 1325.0}]}}None'\n",
    "print(\"마지막 줄 : \" + lastLine)   \n",
    "print(\"읽은 건 수 : \" + str(trade['VwsmTrdarFlpopQq']['list_total_count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a847105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C 드라이브의 볼륨에는 이름이 없습니다.\n",
      " 볼륨 일련 번호: 2C10-A45E\n",
      "\n",
      " C:\\Users\\82107\\Code\\201710748\\src 디렉터리\n",
      "\n",
      "2021-09-22  오후 01:36         2,586,514 ds_practice_data.json\n",
      "               1개 파일           2,586,514 바이트\n",
      "               0개 디렉터리  67,531,153,408 바이트 남음\n"
     ]
    }
   ],
   "source": [
    "!dir src\\ds_practice_data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad41d8f0",
   "metadata": {},
   "source": [
    "# 문제 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b155395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.2\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f02f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "(base) C:\\Users\\82107>pyspark\n",
    "Python 3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "WARNING: An illegal reflective access operation has occurred\n",
    "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/C:/Anaconda3/Lib/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
    "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
    "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
    "WARNING: All illegal access operations will be denied in a future release\n",
    "21/09/18 23:53:06 WARN Shell: Did not find winutils.exe: {}\n",
    "java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
    "        at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)\n",
    "        at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)\n",
    "        at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)\n",
    "        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)\n",
    "        at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)\n",
    "        at org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1814)\n",
    "        at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1791)\n",
    "        at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\n",
    "        at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\n",
    "        at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:302)\n",
    "        at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\n",
    "        at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\n",
    "        at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\n",
    "        at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n",
    "        at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\n",
    "        at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\n",
    "        at org.apache.spark.util.Utils$.createTempDir(Utils.scala:326)\n",
    "        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:343)\n",
    "        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894)\n",
    "        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\n",
    "        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\n",
    "        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\n",
    "        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)\n",
    "        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)\n",
    "        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n",
    "Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
    "        at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)\n",
    "        at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)\n",
    "        at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)\n",
    "        ... 21 more\n",
    "21/09/18 23:53:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "21/09/18 23:53:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.1.2\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.8.8 (default, Apr 13 2021 15:08:03)\n",
    "Spark context Web UI available at http://DESKTOP-1PQHHPS:4041\n",
    "Spark context available as 'sc' (master = local[*], app id = local-1631976788534).\n",
    "SparkSession available as 'spark'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60a4c64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java 11.0.12 2021-07-20 LTS\n",
      "Java(TM) SE Runtime Environment 18.9 (build 11.0.12+8-LTS-237)\n",
      "Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.12+8-LTS-237, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "!java --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44793390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "javac 11.0.12\n"
     ]
    }
   ],
   "source": [
    "!javac --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da32f020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.8\n"
     ]
    }
   ],
   "source": [
    "!python --version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c94f1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114ed998",
   "metadata": {},
   "source": [
    "# 문제 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62f8a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"C:\\\\Anaconda3\\\\python.exe\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"C:\\\\Anaconda3\\\\python.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "468a09b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "myConf=pyspark.SparkConf()\n",
    "#myConf=pyspark.SparkConf().set(\"spark.driver.bindAddress\", \"127.0 .0.1\")\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e2cf010",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ffa2ad0f-dd53-4ed0-984e-9bec09d470bb",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['행정기관,인구수(계),인구수(남),인구수(여),구성비(계),구성비(남),구성비(여),성비,세대수,세대당인구,관리기관명,관리부서명,부서전화번호,데이터기준일자',\n",
       " '의정부1동,32292,16538,15754,6.97,3.57,3.4,104.98,19998,1.61,의정부시,민원여권과,031-828-2466,2021-09-10',\n",
       " '의정부2동,31380,15608,15772,6.77,3.37,3.4,98.96,16410,1.91,의정부시,민원여권과,031-828-2466,2021-09-10',\n",
       " '호원1동,36124,17595,18529,7.8,3.8,4,94.96,15653,2.31,의정부시,민원여권과,031-828-2466,2021-09-10',\n",
       " '호원2동,34957,16923,18034,7.54,3.65,3.89,93.84,13683,2.55,의정부시,민원여권과,031-828-2466,2021-09-10']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popRdd = spark.sparkContext.textFile(os.path.join(\"data\",\"경기도_의정부시_인구현황_20210910.csv\"), use_unicode=True)\n",
    "popRdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8072409",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6c468f81-7ad1-4d5c-bad1-a6577f010ce8",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['연도별,서귀포시 인구수,65세이상 인구수 ,14세이하 인구수,고령화비율,노령화지수,데이터기준일자',\n",
       " '2012,154057,25826,22861,16.76,112.97,2021-08-31',\n",
       " '2013,155641,26936,22393,17.31,120.29,2021-08-31',\n",
       " '2014,158512,27877,22058,17.59,126.38,2021-08-31',\n",
       " '2015,164519,28979,22362,17.61,129.59,2021-08-31']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd1 = spark.sparkContext.textFile(os.path.join(\"data\",\"제주특별자치도_서귀포시_고령화비율및노령화지수현황_20210831.csv\"), use_unicode=True)\n",
    "myRdd1.take(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
